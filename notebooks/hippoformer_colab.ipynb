{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HippoFormer Training on Colab\n",
    "\n",
    "**Setup:** Runtime → Change runtime type → **T4 GPU** (or A100 with Pro+)\n",
    "\n",
    "This notebook trains HippoFormer with Gemma-2B + LoRA on WikiText-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/YOUR_USERNAME/BrainLLM.git\n",
    "%cd BrainLLM\n",
    "\n",
    "# Install with all dependencies\n",
    "!pip install -e \".[all]\" -q\n",
    "\n",
    "# Verify\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HuggingFace Login (for Gemma access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Get your token from: https://huggingface.co/settings/tokens\n",
    "# Also accept Gemma license at: https://huggingface.co/google/gemma-2b\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quick Test (5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hippoformer.config import HippoFormerConfig\n",
    "from hippoformer.model import HippoFormer\n",
    "from hippoformer.train import TrainingArgs, HippoFormerTrainer, create_dataloaders\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Configuration\n",
    "config = HippoFormerConfig(\n",
    "    base_model_name=\"google/gemma-2b\",\n",
    "    freeze_base=True,\n",
    "    use_lora=True,\n",
    ")\n",
    "\n",
    "# Quick test settings\n",
    "args = TrainingArgs(\n",
    "    dataset_name=\"wikitext\",\n",
    "    dataset_config=\"wikitext-2-raw-v1\",\n",
    "    batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_epochs=1,\n",
    "    learning_rate=1e-4,\n",
    "    max_seq_length=512,\n",
    "    output_dir=\"./outputs/quick_test\",\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Creating model...\")\n",
    "model = HippoFormer(config)\n",
    "print(f\"Total parameters: {model.get_num_total_params():,}\")\n",
    "print(f\"Trainable parameters: {model.get_num_trainable_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders with limited samples for quick test\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Limit to 500 samples for quick test\n",
    "MAX_SAMPLES = 500\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=args.max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "train_data = dataset[\"train\"].select(range(min(MAX_SAMPLES, len(dataset[\"train\"]))))\n",
    "val_data = dataset[\"validation\"].select(range(min(100, len(dataset[\"validation\"]))))\n",
    "\n",
    "train_tokenized = train_data.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "val_tokenized = val_data.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "train_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "val_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "train_dataloader = DataLoader(train_tokenized, batch_size=args.batch_size, shuffle=True)\n",
    "eval_dataloader = DataLoader(val_tokenized, batch_size=args.batch_size)\n",
    "\n",
    "print(f\"Train batches: {len(train_dataloader)}\")\n",
    "print(f\"Eval batches: {len(eval_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "trainer = HippoFormerTrainer(model, args, tokenizer)\n",
    "history = trainer.train(train_dataloader, eval_dataloader)\n",
    "\n",
    "print(f\"\\nFinal train loss: {history['train_loss'][-1]:.4f}\")\n",
    "if history['eval_loss']:\n",
    "    print(f\"Final eval loss: {history['eval_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full Training (2-3 hours on T4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training configuration\n",
    "# Uncomment and run for full training\n",
    "\n",
    "'''\n",
    "from hippoformer.config import HippoFormerConfig\n",
    "from hippoformer.model import HippoFormer\n",
    "from hippoformer.train import TrainingArgs, HippoFormerTrainer, create_dataloaders\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "config = HippoFormerConfig(\n",
    "    base_model_name=\"google/gemma-2b\",\n",
    "    freeze_base=True,\n",
    "    use_lora=True,\n",
    ")\n",
    "\n",
    "args = TrainingArgs(\n",
    "    dataset_name=\"wikitext\",\n",
    "    dataset_config=\"wikitext-2-raw-v1\",\n",
    "    batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_epochs=3,\n",
    "    learning_rate=1e-4,\n",
    "    max_seq_length=512,\n",
    "    output_dir=\"./outputs/full_training\",\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = HippoFormer(config)\n",
    "train_dataloader, eval_dataloader = create_dataloaders(tokenizer, args)\n",
    "\n",
    "trainer = HippoFormerTrainer(model, args, tokenizer)\n",
    "history = trainer.train(train_dataloader, eval_dataloader)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to persist outputs\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy outputs to Drive\n",
    "!cp -r ./outputs /content/drive/MyDrive/hippoformer_outputs\n",
    "print(\"Outputs saved to Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "from evaluation.metrics import compute_perplexity\n",
    "from evaluation.datasets import create_eval_dataloader\n",
    "\n",
    "# Load eval data\n",
    "eval_loader = create_eval_dataloader(\n",
    "    \"wikitext-2\",\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=4,\n",
    "    max_samples=500,\n",
    ")\n",
    "\n",
    "# Compute perplexity\n",
    "model.eval()\n",
    "perplexity = compute_perplexity(model, eval_loader, device=\"cuda\")\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ablation (takes longer)\n",
    "'''\n",
    "from evaluation.ablation import AblationRunner\n",
    "from hippoformer.config import HippoFormerConfig\n",
    "\n",
    "base_config = HippoFormerConfig(\n",
    "    base_model_name=\"google/gemma-2b\",\n",
    "    freeze_base=True,\n",
    "    use_lora=True,\n",
    ")\n",
    "\n",
    "runner = AblationRunner(\n",
    "    base_config=base_config,\n",
    "    output_dir=\"./outputs/ablation\",\n",
    "    num_seeds=3,\n",
    ")\n",
    "\n",
    "# Run key ablations\n",
    "variants = [\"baseline\", \"no_salience\", \"no_memory\", \"no_drift\"]\n",
    "results = runner.run_ablation_suite(variants)\n",
    "runner.generate_comparison_table()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
